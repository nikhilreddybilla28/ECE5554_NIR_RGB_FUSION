<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>RGB-NIR Fusions</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h4 class="title is-1 publication-title">Robust Autonomous Navigation in Adverse Weather Conditions Using RGB-NIR Image Fusion</h4>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://nikhilreddybilla28.github.io/" target="_blank">Nikhil Reddy Billa</a>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Akash Reddy Mallepally</a>,</span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Bradley Department of Electrical and Computer Engineering<br>Virginia Tech</span>
              
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link 
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>-->

                    <!-- Supplementary PDF link
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/nikhilreddybilla28/rgb_nir_fusion" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link 
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>-->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video 
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>-->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>

            This project proposes a solution by fusing RGB (Red, Green, Blue) and NIR (Near-Infrared) imaging to enhance autonomous navigation in challenging weather conditions. RGB-NIR image fusion leverages the complementary strengths of visible and infrared light, providing richer information for scene understanding. 
            By using the IDD-AW dataset and training a state-of-the-art segmentation model with fused modalities, we achieved a significant improvement in segmentation performance, with an mIoU increase of 4% compared to RGB-only model. Our results also indicate that a significant performance gains can be achieved with the by implemented more advanced image fusion techniques.
            <!-- In recent years, there have been several incidents that have exposed the drawbacks of autonomous systems when operating in adverse weather conditions. Heavy rain, dense fog, lowlight, glare and snow have all caused ML models to mispredict, making it harder for these systems to make correct decisions and navigate safely. In snowy conditions, for example, self-driving cars have struggled to detect pedestrians and obstacles, while in fog, some systems have been completely unable to "see" their surroundings. These issues highlight the urgent need for more resilient technology that can handle the unpredictability of harsh weather, ensuring that autonomous navigation remains safe and reliable no matter the conditions.

            This project proposes a solution by fusing RGB (Red, Green, Blue) and NIR (Near-Infrared) imaging to enhance autonomous navigation in challenging weather conditions. RGB-NIR image fusion leverages the complementary strengths of visible and infrared light, providing richer information for scene understanding.
            
            The proposed method is evaluated through simulations and real-world tests under various adverse weather conditions, demonstrating a significant improvement in accuracy and reliability compared to traditional RGB-based systems. This research aims to advance the development of autonomous systems capable of safely navigating in all weather conditions, with applications in autonomous vehicles, drones, and other robotic platforms used in safety-critical tasks.
          -->
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/iddaw_bg.png" alt="RGB-NIR CARISION"/>
        <h2 class="subtitle has-text-centered">
          RGB-NIR COMPARISION.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/00000024_rgb.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          FOGGY Conditions.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/00000009_rgb.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         SNOW Conditions.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/00000014_rgb.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
       LOWLIGHT Conditions.
     </h2>
   </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/00000000_rgb.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        RAINY Conditions.
      </h2>
    </div>
  </div>
</div>
</div>
</section>


<!-- End image carousel -->


<!-- Problem Statement -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            In recent years, there have been several incidents that have exposed the drawbacks of autonomous systems when operating in adverse weather conditions. Heavy rain, dense fog, lowlight, glare and snow have all caused ML models to mispredict, making it harder for these systems to make correct decisions and navigate safely. In snowy conditions, for example, self-driving cars have struggled to detect pedestrians and obstacles, while in fog, some systems have been completely unable to "see" their surroundings. 
            Current autonomous navigation systems are often unreliable in adverse weather conditions such as rain, fog, and snow, where visibility is reduced, and sensors struggle to perform effectively. Traditional RGB  imaging alone is not sufficient to handle these challenges, leading to errors in detecting drive scenes and making decisions.
            To improve perception in such difficult environments, a better approach is needed. Combining RGB imaging with Near-Infrared (NIR) images which are less impacted by poor visibility, can provide a clearer and more reliable view of the surroundings. The Near-Infrared range of light, having greater wavelengths than the visible range, can penetrate more through suspended particles in the camera’s view. This project aims to develop a solution that uses RGB-NIR fusion to enhance the accuracy, safety, and reliability of autonomous navigation systems in challenging weather conditions.

          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Problem Statement -->

<!-- Approach -->
<section class="section hero is-dark">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Approach</h2>
        <div class="content has-text-justified">
          <p>
            <li><strong>Data Preparation:</strong> Utilize the IDD-AW dataset, which contains 5000 paired RGB and NIR images with pixel-level annotations across rain, fog, snow, and low-light conditions.</li>
            <li><strong>Data Fusion:</strong> Fuse RGB and NIR images through early fusion (stacking channels) and evaluate this representation in the segmentation pipeline.</li>
            <li><strong>Model Architecture:</strong> Train the InternImage segmentation model with RGB, NIR, and fused inputs.</li>
            <li><strong>Code Utilization:</strong> The code is developed up on publicly available implementations of mmsegmentation and InternImage. Custom data loader with four channeled input and fusion preprocessing was developed.</li>

          </p>
          <p>
            To ensure accuracy, image alignment will be applied to validate image pairs RGB and NIR images are fused, combining detailed color information of RGB with NIR’s ability to enhance visibility in adverse weather conditions.
            
            To gauge the efficacy of fused RGB-NIR images, we will compare the improvement in image segmentation for a variety of adverse weather conditions individually.
            <ul>
              <li>Night/Low-light</li>
              <li>Rain</li>
              <li>Snow</li>
              <li>Fog/mist/</li>
              
          </ul>
        </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Experimental Setup 
<section class="section hero is-dark">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Setup</h2>
        <div class="content has-text-justified">
          <p>
            This project focuses on enhancing autonomous navigation in adverse weather using RGB-NIR image fusion. The approach begins with data acquisition from the IDDAW [1] and Dark Vision Dataset (DVD) [2], capturing driving scenes under poor lighting and harsh weather conditions. To ensure accuracy, image alignment will be applied to validate image pairs
            RGB and NIR images are fused using multispectral techniques, combining detailed color information with NIR’s ability to enhance visibility in low-light conditions. The fused images are then processed using deep learning-based segmentation models like DeepLab and InternImage, to identify key elements in drive scenes.

            To gauge the efficacy of fused RGB-NIR images, we will compare the improvement in image segmentation for a variety of adverse weather conditions individually.
            <ul>
              <li>Low-light conditions</li>
              <li>Rain</li>
              <li>Snow</li>
              <li>Fog/mist/haze</li>
              <li>Glare from the sun or other sources</li>
          </ul>
            Another important consideration is the generalizability of the model trained on location specific datasets. In our case, the IDDAW and the Dark Vision datasets are based on the Indian and German road and traffic scenes.

            Finally, the system will be tested and validated in both real-world and simulated environments, comparing its performance to traditional RGB-based models. This approach aims to significantly improve navigation accuracy and reliability in challenging weather conditions.

          </p>
        </div>
      </div>
    </div>
  </div>
</section End Experimental Setup -->

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments and Results</h2>
        <div class="content has-text-justified">
        <h3> Experimental Setup</h3>
          <p>
            <ul>
                <li><strong>Dataset:</strong> IDD-AW with a 70-20-10 train-validation-test split.</li>
                <li><strong>Metrics:</strong> mIoU (Mean Intersection over Union).</li>
                <li><strong>Baselines:</strong> Compared cityscapes RGB, IDDAW RGB-only and IDDAW NIR-only segmentation with the fused approach.</li>
            </ul>
          </p>
  
        <h3>Detailed Results</h3>
        <br>
        <h4>Comparision on mIoU of Cityscapes and IDD-AW(RGB/NIR/RGB+NIR)</h4>
        <table>
            <thead>
                
                <tr>
                    <th>Test → <br> Train ↓</th>
                    <th>CS RGB </th>
                    <th>IDDAW *</th>
              
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>CS RGB</td>
                    <td>82.1</td>
                    <td>42.2</td>
                    
                </tr>
                <tr>
                    <td>IDD-AW RGB</td>
                    <td>47.8</td>
                    <td>62.2</td>
                    
                </tr>
                <tr>
                    <td>IDD-AW NIR</td>
                    <td>-</td>
                    <td>60.0</td>
                    
                </tr>
                <tr>
                    <td>IDD-AW NIR+RGB</td>
                    <td>-</td>
                    <td><b>66.7</b></td>
                    
                </tr>
            </tbody>
        </table>



  <h4>Comparision for each Weather Condition</h4>
  <p>
    This table presents a detailed breakdown of segmentation performance across different weather conditions, showcasing the effectiveness of RGB, NIR, and RGB-NIR fused modalities in rain, fog, low light, and snow. The results highlight the robustness of RGB-NIR fusion in adverse weather, demonstrating consistently higher mIoU scores across all weather conditions.
</p>
  <table>
    <thead>
        <tr>
            <th>Test → <br> Train ↓</th>
            <th>Rain</th>
            <th>Fog</th>
            <th>Lowlight</th>
            <th>Snow</th>
        </tr>
    </thead>

    <tbody>
        <tr>
            <td>IDD-AW RGB</td>
            <td>61.5</td>
            <td>63.8</td>
            <td>61.7</td>
            <td>52.1</td>
            
        </tr>

        <tr>
            <td>IDD-AW NIR</td>
            <td>60.3</td>
            <td>58.2</td>
            <td>56.7</td>
            <td>50.2</td>
            
        </tr>
        <tr>
            <td>IDD-AW NIR+RGB</td>
            <td><b>65.9</b></td>
            <td><b>64.8</b></td>
            <td><b>62.4</b></td>
            <td><b>52.5</b></td>
            
        </tr>
    </tbody>
</table>
</section>



<section class="section hero is-dark">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Results</h2>
        <div class="content has-text-justified">
        
          <p>
              <b>Visual comparisons of segmentation outputs</b>
          </p>
          <ul>
              <li><strong>Success Cases:</strong> Improved pedestrian segmentation in all weather conditions with RGB-NIR fusion and enhanced vehicle detection in low light due to NIR.</li>
              <li><strong>Failure Cases:</strong> Misclassification of overlapping objects under  snow and inconsistent performance for distant objects.</li>
          </ul>
          
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/iddaw_bg.png" alt="Qualitative Results: Segmentation Comparisons of NIR/RGB/FUSED" />
        <h4 class="subtitle has-text-centered">
          Examples of input images, NIR-only predictions, RGB-only predictions, fused predictions, and Ground Truth
        </h4>
      </div>
  </div>
</div>
</div>
</section>

<section id="conclusion">
  <h3>Conclusion</h3>
  <p>
      This report described a novel approach to enhancing semantic segmentation in adverse weather conditions through RGB-NIR fusion. By leveraging the IDD-AW dataset and evaluating results, we demonstrated the significant benefits of multi-modal data for robust and safe segmentation. Future work includes optimizing fusion strategies and exploring advanced fusion techniques and loss functions tailored for segmentation in adverse weather conditions.
  </p>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">References</h1>
        <div class="content has-text-justified">
  
        <p>
        <ul>
          <li>A Review on Deep Learning Techniques Applied to Semantic Segmentation <a href="https://arxiv.org/abs/1704.06857" target="_blank"></a></li>
          <li>DD-AW: A Benchmark for Safe and Robust Segmentation of Drive Scenes in Unstructured Traffic and Adverse Weather <a href="https://paperswithcode.com/paper/idd-aw-a-benchmark-for-safe-and-robust" target="_blank"></a></li>
          
          <li>Code developed upon <a href="https://github.com/OpenGVLab/InternImage" target="_blank">InternImage codebase</a></li>
          <li>https://github.com/open-mmlab/mmsegmentation/tree/v0.27.0</li>
        </ul>
        </p>
</section>


<!-- Success factors 
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Success factors</h2>
        <div class="content has-text-justified">
          <p>
            <li>Gain insights into the improvement of image segmentation for a variety of individual adverse conditions</li>
            <li>Highlight why some conditions are better dealt with than others</li>
            <li>Gauge the overall efficacy of RGB-NIR fused images across conditions and datasets (Cross domain improvement using domanin adaptation)</li>
            <li>Understand the factors responsible for the model's change in performance across datasets from starkly different locations</li>
            <li>Potentially capturing and infusing other wavelengths of light, i.e. introducing more channels into the image, to test the model's boost in accurate segmentation</li>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
End Success factors -->





<!-- Video carousel 
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
             Your video file here 
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">-->
            <!-- Your video file here
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
            <!-- Your video file here 
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->


<!--BibTex citation 
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @misc{moosavidezfooli2016deepfoolsimpleaccuratemethod,
        title={DeepFool: a simple and accurate method to fool deep neural networks}, 
        author={Seyed-Mohsen Moosavi-Dezfooli and Alhussein Fawzi and Pascal Frossard},
        year={2016},
        eprint={1511.04599},
        archivePrefix={arXiv},
        primaryClass={cs.LG},
        url={https://arxiv.org/abs/1511.04599},
        }

        @inproceedings{shaik2024idd,
          title={IDD-AW: A Benchmark for Safe and Robust Segmentation of Drive Scenes in Unstructured Traffic and Adverse Weather},
          author={Shaik, Furqan Ahmed and Reddy, Abhishek and Billa, Nikhil Reddy and Chaudhary, Kunal and Manchanda, Sunny and Varma, Girish},
          booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
          pages={4614--4623},
          year={2024}
        }
      
      </code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
